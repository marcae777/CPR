{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/cpr/Nivel.py\", line 14, in <module>\n",
      "    from mpl_toolkits.basemap import Basemap\n",
      "ImportError: No module named basemap\n"
     ]
    }
   ],
   "source": [
    "%%python2\n",
    "from cpr.Nivel import Nivel\n",
    "from cpr.SqlDb import SqlDb\n",
    "import cpr.information as info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_existing_table(self,table_name):\n",
    "    '''\n",
    "    inserts data into SQL table from list of fields and values\n",
    "    Parameters\n",
    "    ----------\n",
    "    table_name   = SQL db table name\n",
    "    Returns\n",
    "    -------\n",
    "    Sql sentence,str\n",
    "    '''\n",
    "    df = self.read_sql('describe %s'%table_name)\n",
    "    df['Null'][df['Null']=='NO'] = 'NOT NULL'\n",
    "    df['Null'][df['Null']=='YES'] = 'NULL'\n",
    "    sentence = 'CREATE TABLE %s ('%table_name\n",
    "    if df[df['Extra']=='auto_increment'].empty:\n",
    "        pk = None\n",
    "    else:\n",
    "        pk = df[df['Extra']=='auto_increment']['Field'].values[0]\n",
    "    for id,serie in df.iterrows():\n",
    "        if (serie.Default=='0') or (serie.Default is None):\n",
    "            row = '%s %s %s'%(serie.Field,serie.Type,serie.Null)\n",
    "        else:\n",
    "            if (serie.Default == 'CURRENT_TIMESTAMP'):\n",
    "                serie.Default = \"DEFAULT %s\"%serie.Default\n",
    "            elif serie.Default == '0000-00-00':\n",
    "                serie.Default = \"DEFAULT '1000-01-01 00:00:00'\"\n",
    "            else:\n",
    "                serie.Default = \"DEFAULT '%s'\"%serie.Default\n",
    "            row = '%s %s %s %s'%(serie.Field,serie.Type,serie.Null,serie.Default)\n",
    "        if serie.Extra:\n",
    "            row += ' %s,'%serie.Extra\n",
    "        else:\n",
    "            row += ',' \n",
    "        sentence+=row\n",
    "    if pk:\n",
    "        sentence +='PRIMARY KEY (%s)'%pk\n",
    "    else:\n",
    "        sentence = sentence[:-1]\n",
    "    sentence +=');'\n",
    "    return sentence\n",
    "\n",
    "def siata_remote_data_to_transfer(start,end):\n",
    "    remote = cpr.Nivel(**cpr.info.REMOTE)\n",
    "    codigos_str = '('+str(list(self.infost.index)).strip('[]')+')'\n",
    "    df = remote.read_sql('SELECT * FROM datos WHERE cliente in %s and %s'%(codigos_str,self.fecha_hora_query(start,end)))\n",
    "    return df\n",
    "\n",
    "def default_values(self,table_name):\n",
    "    describe_table = self.read_sql('describe %s'%table_name)\n",
    "    not_null = describe_table['Default'].notnull()\n",
    "    default_values = describe_table[['Field','Default','Type']][not_null].set_index('Field')[['Default','Type']]\n",
    "    return default_values\n",
    "\n",
    "def filter_data_to_update(table_name,path):\n",
    "    default = default_values(self,table_name)\n",
    "    if default[default['Type']=='time'].empty:\n",
    "        time_fields = None\n",
    "    else:\n",
    "        time_fields = default[default['Type']=='time']\n",
    "    for index,s in time_fields.iterrows():\n",
    "        df = pd.read_csv(path,index_col=0)\n",
    "        df[index] = df[index].apply(lambda x:str(x)[6:15])\n",
    "    return df.applymap(lambda x:str(x))\n",
    "\n",
    "def insert_in_datos(path):\n",
    "    inicia = datetime.datetime.now()\n",
    "    table_name = 'datos'\n",
    "    df = filter_data_to_update(table_name,path)\n",
    "    query = 'INSERT INTO %s '%table_name+'('+str(list(df.columns)).strip('[]').replace(\"'\",'')+') VALUES '\n",
    "    for id,s in df.iterrows():\n",
    "        query+=('('+str(list(s.values)).strip('[]'))+'), '\n",
    "    query = query[:-2]+' ON DUPLICATE KEY UPDATE '\n",
    "    describe = self.read_sql('describe %s;'%table_name)\n",
    "    not_primary_keys = describe[describe['Key']!='PRI']\n",
    "    if not_primary_keys.empty:\n",
    "        not_primary_keys = describe.Field.values\n",
    "    else:\n",
    "        not_primary_keys = not_primary_keys.Field.values\n",
    "    for key in not_primary_keys:\n",
    "        query+=('%s = VALUES(%s), '%(key,key))\n",
    "    query=query[:-2]\n",
    "    finaliza = datetime.datetime.now()\n",
    "    self.user = 'root'\n",
    "    self.passwd = 'chan104jak'\n",
    "    self.execute_sql('SET GLOBAL max_allowed_packet=1073741824;')\n",
    "    self.execute_sql(query)\n",
    "    print(finaliza - inicia)\n",
    "    \n",
    "#os.system(\"scp -r mcano@siata.gov.co:data_migration/* ../../cprweb/src/media/data_migration/\")\n",
    "\n",
    "self = SqlDb(**info.REMOTE)\n",
    "\n",
    "folder_path = '/home/alexa/Repositorios/cprweb/src/media/weekly_data/'\n",
    "files = os.listdir(folder_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in files:\n",
    "    insert_in_datos(folder_path+path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = Nivel(codigo=99,**info.LOCAL).level('2018-03-01',datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:40.495675\n",
      "0:00:39.337454\n",
      "0:00:40.418218\n",
      "0:00:40.114404\n",
      "0:00:39.968619\n"
     ]
    }
   ],
   "source": [
    "self = SqlDb(**info.REMOTE)\n",
    "folder_path = '/home/alexa/Repositorios/cprweb/src/media/weekly_data/'\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "april = []\n",
    "for path in files:\n",
    "    if path[:20] == 'weekly_level_2018-04':\n",
    "        april.append(path)\n",
    "\n",
    "date_offset = pd.to_datetime(level.dropna().index[-1])\n",
    "date = date_offset-pd.to_datetime(pd.read_csv(folder_path + april[0],index_col=0)['fecha'][0])\n",
    "for path in april:\n",
    "    raw_data = pd.read_csv(folder_path + path,index_col=0)\n",
    "    raw_data['fecha'] = (pd.to_datetime(raw_data['fecha']) + datetime.timedelta(days=date.days)).apply(lambda x:x.strftime('%Y-%m-%d'))\n",
    "    raw_data.to_csv(folder_path+'future_'+path)\n",
    "    insert_in_datos(folder_path+'future_'+path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
